{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30039ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bc9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from scipy.stats import sem\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09620438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_D</th>\n",
       "      <th>FV_M</th>\n",
       "      <th>FV_DATE</th>\n",
       "      <th>VISIT_CODE</th>\n",
       "      <th>WP_ID</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>RegCode</th>\n",
       "      <th>INDCode</th>\n",
       "      <th>HICCode</th>\n",
       "      <th>HICPerCode</th>\n",
       "      <th>E1524Code</th>\n",
       "      <th>E2554Code</th>\n",
       "      <th>E55pCode</th>\n",
       "      <th>ConCode</th>\n",
       "      <th>Claims_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>2019-06-29 2093649</td>\n",
       "      <td>2093649</td>\n",
       "      <td>812115</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>2019-02-25 1095872</td>\n",
       "      <td>1095872</td>\n",
       "      <td>812330</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>2019-06-19 2091294</td>\n",
       "      <td>2091294</td>\n",
       "      <td>812190</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>2019-05-06 2029237</td>\n",
       "      <td>2029237</td>\n",
       "      <td>238990</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>2019-10-25 2125435</td>\n",
       "      <td>2125435</td>\n",
       "      <td>236110</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FV_D  FV_M     FV_DATE          VISIT_CODE    WP_ID   NAICS  RegCode  \\\n",
       "0    29     6  2019-06-29  2019-06-29 2093649  2093649  812115        3   \n",
       "1    25     2  2019-02-25  2019-02-25 1095872  1095872  812330        2   \n",
       "2    19     6  2019-06-19  2019-06-19 2091294  2091294  812190        2   \n",
       "3     6     5  2019-05-06  2019-05-06 2029237  2029237  238990        4   \n",
       "4    25    10  2019-10-25  2019-10-25 2125435  2125435  236110        4   \n",
       "\n",
       "   INDCode  HICCode  HICPerCode  E1524Code  E2554Code  E55pCode  ConCode  \\\n",
       "0       10        1           2          1          1         2        1   \n",
       "1       10        2           2          1          1         2        4   \n",
       "2       10        2           2          1          1         2        4   \n",
       "3        4        5           5          2          2         2        1   \n",
       "4        4        5           5          2          2         2        3   \n",
       "\n",
       "   Claims_Code  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            3  \n",
       "4            3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the data --> this file represents only the train set\n",
    "data = pd.read_csv(\"Train_set_CC.csv\", encoding = \"latin-1\")\n",
    "del data[\"Unnamed: 0\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad39081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19070 entries, 0 to 19069\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   FV_D         19070 non-null  int64 \n",
      " 1   FV_M         19070 non-null  int64 \n",
      " 2   FV_DATE      19070 non-null  object\n",
      " 3   VISIT_CODE   19070 non-null  object\n",
      " 4   WP_ID        19070 non-null  int64 \n",
      " 5   NAICS        19070 non-null  int64 \n",
      " 6   RegCode      19070 non-null  int64 \n",
      " 7   INDCode      19070 non-null  int64 \n",
      " 8   HICCode      19070 non-null  int64 \n",
      " 9   HICPerCode   19070 non-null  int64 \n",
      " 10  E1524Code    19070 non-null  int64 \n",
      " 11  E2554Code    19070 non-null  int64 \n",
      " 12  E55pCode     19070 non-null  int64 \n",
      " 13  ConCode      19070 non-null  int64 \n",
      " 14  Claims_Code  19070 non-null  int64 \n",
      "dtypes: int64(13), object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcce6a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FV_D           0\n",
       "FV_M           0\n",
       "FV_DATE        0\n",
       "VISIT_CODE     0\n",
       "WP_ID          0\n",
       "NAICS          0\n",
       "RegCode        0\n",
       "INDCode        0\n",
       "HICCode        0\n",
       "HICPerCode     0\n",
       "E1524Code      0\n",
       "E2554Code      0\n",
       "E55pCode       0\n",
       "ConCode        0\n",
       "Claims_Code    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d87507",
   "metadata": {},
   "source": [
    "Split data into 70:30 train / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4fec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FV_D', 'FV_M', 'WP_ID', 'NAICS', 'RegCode', 'INDCode', 'HICCode', 'HICPerCode', 'E1524Code', 'E2554Code', 'E55pCode', 'ConCode']\n",
      "(19070, 12) (19070,)\n",
      "4    4527\n",
      "1    4089\n",
      "2    4063\n",
      "5    3377\n",
      "3    3014\n",
      "Name: Claims_Code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#using the train set define the features (X) and target (y) --> using Claims_Code as dependent variable\n",
    "#includes all features\n",
    "\n",
    "X = data.drop(['FV_DATE', 'VISIT_CODE','Claims_Code'], axis=1)\n",
    "y = data['Claims_Code']\n",
    "\n",
    "print(list(X.columns.values))  #Features used in the model\n",
    "print(X.shape, y.shape)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d41f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RegCode', 'INDCode', 'HICCode', 'E2554Code']\n"
     ]
    }
   ],
   "source": [
    "#using the train set define the features (X) and target (y) --> using Claims_Code as dependent variable\n",
    "#features identified during Feature Selection via RFE 1 -->  RegCode, INDCode, HICCode, E2554Code\n",
    "\n",
    "X = data.drop(['FV_D', 'FV_M','FV_DATE', 'VISIT_CODE', 'WP_ID', 'NAICS', 'HICPerCode','E1524Code', 'E55pCode','ConCode','Claims_Code'], axis=1)\n",
    "y = data['Claims_Code']\n",
    "\n",
    "print(list(X.columns.values))  #Features used in the model\n",
    "print(X.shape, y.shape)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the train set define the features (X) and target (y) --> using Claims_Code as dependent variable\n",
    "#features identified during Feature Selection via chi^2 method -->  'WP_ID', 'NAICS','INDCode', 'HICCode', 'HICPerCode'\n",
    "X = data.drop(['FV_D', 'FV_M', 'FV_DATE', 'VISIT_CODE', 'RegCode', 'E2554Code', 'E1524Code', 'E55pCode','ConCode', 'Claims_Code'], axis=1)\n",
    "y = data['Claims_Code']\n",
    "\n",
    "print(list(X.columns.values))  #Features used in the model\n",
    "print(X.shape, y.shape)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356d3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y,  train_size = .70, random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac7d17",
   "metadata": {},
   "source": [
    "RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cfacdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd42d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "#configure the function for the Random Forest Model\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, random_state=23)\n",
    "RF_model = RandomForestClassifier()  #default of 100 trees\n",
    "scorer = 'accuracy'\n",
    "print(RF_model.fit(X_tr, y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e6d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "score = np.mean(cross_val_score(RF_model, X, y, scoring = 'accuracy', cv=cv))\n",
    "print('Accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effc96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_features': 4, 'min_samples_leaf': 1, 'n_estimators': 50}\n",
      "Best accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "#fine tuning parameters for the model --> maximum number of features, minimum number of samples in a leaf, number of estimators\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_features = [X.shape[1]//3, 'sqrt', 'log2', 'auto']\n",
    "min_samples_leaf = [1, 10, 30]\n",
    "n_estimators = [50, 100, 300]\n",
    "search_grid = {'n_estimators':n_estimators, 'max_features': max_features, 'min_samples_leaf': min_samples_leaf}\n",
    "search_func = GridSearchCV(estimator=RF_model, param_grid=search_grid, scoring=scorer, cv=cv)\n",
    "\n",
    "search_func.fit(X, y)\n",
    "best_params = search_func.best_params_\n",
    "best_score = search_func.best_score_\n",
    "\n",
    "print('Best parameters: %s' % best_params)\n",
    "print('Best accuracy: %.3f' % best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea72787",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = RF_model.predict(X_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da22206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1248    0    0    0    0]\n",
      " [   0 1210    0    0    0]\n",
      " [   0    0  902    0    0]\n",
      " [   0    0    0 1387    0]\n",
      " [   0    0    0    0  974]]\n"
     ]
    }
   ],
   "source": [
    "#Create confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "con_matx = confusion_matrix(y_ts, y_predicted)\n",
    "print(con_matx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad098f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      1248\n",
      "           2       1.00      1.00      1.00      1210\n",
      "           3       1.00      1.00      1.00       902\n",
      "           4       1.00      1.00      1.00      1387\n",
      "           5       1.00      1.00      1.00       974\n",
      "\n",
      "    accuracy                           1.00      5721\n",
      "   macro avg       1.00      1.00      1.00      5721\n",
      "weighted avg       1.00      1.00      1.00      5721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2dcf4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -> 1 mean=1.0000 se=0.000\n",
      "Run -> 2 mean=1.0000 se=0.000\n",
      "Run -> 3 mean=1.0000 se=0.000\n",
      "Run -> 4 mean=1.0000 se=0.000\n",
      "Run -> 5 mean=1.0000 se=0.000\n",
      "Run -> 6 mean=1.0000 se=0.000\n",
      "Run -> 7 mean=1.0000 se=0.000\n",
      "Run -> 8 mean=1.0000 se=0.000\n",
      "Run -> 9 mean=1.0000 se=0.000\n",
      "Run -> 10 mean=1.0000 se=0.000\n"
     ]
    }
   ],
   "source": [
    "# create function to evaluate the Repeated k-fold model\n",
    "def mod_eval(X, y, Reps):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=Reps, random_state=23)\n",
    "    model = RandomForestClassifier(random_state=23)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "\n",
    "# configurations to test\n",
    "Reps = range(1,11)\n",
    "results = list()\n",
    "for r in Reps:\n",
    "    scores = mod_eval(X, y, r)\n",
    "    print('Run -> %d mean=%.4f se=%.3f' % (r, mean(scores), stats.sem(scores)))\n",
    "    results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "490ee1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average accuracy for the Repeated k-fold model\n",
    "\n",
    "plt.boxplot(results, labels=[str(r) for r in Reps], showmeans=True)\n",
    "plt.title('Accuracy for Repeated k-fold model')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a9f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -> 1 mean=1.0000 se=0.000\n",
      "Run -> 2 mean=1.0000 se=0.000\n",
      "Run -> 3 mean=1.0000 se=0.000\n",
      "Run -> 4 mean=1.0000 se=0.000\n",
      "Run -> 5 mean=1.0000 se=0.000\n",
      "Run -> 6 mean=1.0000 se=0.000\n",
      "Run -> 7 mean=1.0000 se=0.000\n",
      "Run -> 8 mean=1.0000 se=0.000\n",
      "Run -> 9 mean=1.0000 se=0.000\n",
      "Run -> 10 mean=1.0000 se=0.000\n"
     ]
    }
   ],
   "source": [
    "# create function to evaluate the Stratified repeated k-fold model\n",
    "def mod_eval(X, y, Reps):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=Reps, random_state=23)\n",
    "    model = RandomForestClassifier(random_state=23)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "\n",
    "# configurations to test\n",
    "Reps = range(1,11)\n",
    "results = list()\n",
    "for r in Reps:\n",
    "    scores = mod_eval(X, y, r)\n",
    "    print('Run -> %d mean=%.4f se=%.3f' % (r, mean(scores), stats.sem(scores)))\n",
    "    results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41fd88f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average accuracy stratified model\n",
    "\n",
    "plt.boxplot(results, labels=[str(r) for r in Reps], showmeans=True)\n",
    "plt.title('Accuracy for Repeated Stratified model k-fold model')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0bfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
